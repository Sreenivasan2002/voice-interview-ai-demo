<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Voice Interview AI Demo - Fixed</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 20px; max-width: 900px; line-height: 1.6; background: #f5f5f5; }
    .container { background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
    #status { color: #0066cc; font-weight: bold; margin: 15px 0; min-height: 1.5em; }
    #metrics { color: #555; font-size: 14px; margin: 10px 0; padding: 8px; background: #f0f0f0; border-radius: 4px; }
    button { padding: 10px 20px; margin: 5px 5px 10px 0; font-size: 16px; cursor: pointer; background: #0066cc; color: white; border: none; border-radius: 4px; }
    button:disabled { background: #999; cursor: not-allowed; }
    .filler { color: #e67e22; font-style: italic; font-weight: bold; margin: 10px 0; padding: 8px; background: #fff3e0; border-left: 4px solid #e67e22; border-radius: 4px; }
    #conversation { background: #f9f9f9; padding: 15px; border-radius: 4px; min-height: 60px; margin: 10px 0; }
    #summary { background: #e3f2fd; padding: 15px; border-radius: 4px; min-height: 60px; margin: 10px 0; }
  </style>

  <script type="importmap">
  {
    "imports": {
      "@xenova/transformers": "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2"
    }
  }
  </script>
</head>
<body>
  <div class="container">
    <h1>Voice Interview AI Demo</h1>
    <p>Goal: &lt;30â€“100 MB model, &lt;50 ms feel, real-time summaries + fillers (offline)</p>

    <button id="loadBtn">Load Models</button>
    <button id="startMicBtn" disabled>Start Mic & VAD</button>
    <button id="stopMicBtn" disabled>Stop Mic</button>

    <div id="status">Status: Waiting...</div>
    <div id="metrics"></div>

    <div id="fillerArea" class="filler"></div>

    <h3>Live Transcription:</h3>
    <div id="conversation">Speak something...</div>

    <h3>Live Summary:</h3>
    <div id="summary">Summary will appear after you speak and pause...</div>
  </div>

  <script type="module">
    import { pipeline, env } from '@xenova/transformers';

    env.logLevel = 'silent';
    env.allowLocalModels = false;

    const status = document.getElementById('status');
    const metrics = document.getElementById('metrics');
    const convDiv = document.getElementById('conversation');
    const summaryDiv = document.getElementById('summary');
    const fillerDiv = document.getElementById('fillerArea');

    const loadBtn = document.getElementById('loadBtn');
    const startMicBtn = document.getElementById('startMicBtn');
    const stopMicBtn = document.getElementById('stopMicBtn');

    let transcriber = null;
    let summarizer = null;
    let audioContext = null;
    let stream = null;
    let mediaRecorder = null;
    let audioChunks = [];
    let lastSpeechTime = Date.now();
    let fillerCooldown = false;
    let fullConversation = '';
    let isProcessing = false;
    let silenceTimer = null;

    loadBtn.addEventListener('click', async () => {
      loadBtn.disabled = true;
      status.textContent = 'Loading Whisper tiny + T5 small...';

      try {
        // Load Whisper tiny for transcription (~40MB)
        status.textContent = 'Loading Whisper tiny (~40MB)...';
        transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', {
          quantized: true
        });

        // Load T5 for summarization
        status.textContent = 'Loading T5 small summarizer...';
        summarizer = await pipeline('summarization', 'Xenova/t5-small', {
          quantized: true
        });

        status.textContent = 'âœ… Models loaded! Click Start Mic.';
        startMicBtn.disabled = false;
      } catch (err) {
        status.textContent = 'âŒ Load failed: ' + err.message;
        console.error(err);
        loadBtn.disabled = false;
      }
    });

    startMicBtn.addEventListener('click', async () => {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Use MediaRecorder for better audio capture
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };

        mediaRecorder.onstop = async () => {
          if (audioChunks.length > 0 && !isProcessing) {
            await processAudio();
          }
          audioChunks = [];
        };

        // Start recording in chunks
        mediaRecorder.start();

        // Simple VAD: detect silence and process
        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        source.connect(analyser);

        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        function checkAudio() {
          if (!stream || !stream.active) return;

          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;

          if (average > 10) {
            // Speech detected
            lastSpeechTime = Date.now();
            if (silenceTimer) {
              clearTimeout(silenceTimer);
              silenceTimer = null;
            }
          } else {
            // Silence detected
            const silenceMs = Date.now() - lastSpeechTime;
            
            if (silenceMs > 2000 && !silenceTimer && !isProcessing) {
              silenceTimer = setTimeout(async () => {
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                  mediaRecorder.stop();
                  
                  if (!fillerCooldown) {
                    speakFiller("Let me think about that...");
                    fillerCooldown = true;
                    setTimeout(() => fillerCooldown = false, 5000);
                  }
                  
                  // Restart recording after processing
                  setTimeout(() => {
                    if (mediaRecorder && stream && stream.active) {
                      audioChunks = [];
                      mediaRecorder.start();
                    }
                  }, 100);
                }
                silenceTimer = null;
              }, 500);
            }
          }

          requestAnimationFrame(checkAudio);
        }

        checkAudio();

        status.textContent = 'ðŸŽ¤ Microphone active | Speak and pause for transcription';
        startMicBtn.disabled = true;
        stopMicBtn.disabled = false;
      } catch (err) {
        status.textContent = 'âŒ Mic error: ' + err.message;
        console.error(err);
      }
    });

    stopMicBtn.addEventListener('click', () => {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
      if (stream) stream.getTracks().forEach(track => track.stop());
      if (audioContext) audioContext.close();

      status.textContent = 'Mic stopped.';
      startMicBtn.disabled = false;
      stopMicBtn.disabled = true;
    });

    async function processAudio() {
      if (isProcessing || audioChunks.length === 0) return;
      
      isProcessing = true;
      const startTime = Date.now();
      status.textContent = 'ðŸ”„ Transcribing...';

      try {
        // Convert audio blob to array buffer
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        const arrayBuffer = await audioBlob.arrayBuffer();

        // Decode audio
        const audioContextForDecode = new AudioContext({ sampleRate: 16000 });
        const audioBuffer = await audioContextForDecode.decodeAudioData(arrayBuffer);
        const audioData = audioBuffer.getChannelData(0);

        // Transcribe
        const transcribeStart = Date.now();
        const result = await transcriber(audioData, {
          chunk_length_s: 30,
          stride_length_s: 5,
          return_timestamps: false
        });
        const transcribeTime = Date.now() - transcribeStart;

        const transcription = result.text.trim();
        
        if (transcription && transcription.length > 3) {
          fullConversation += (fullConversation ? ' ' : '') + transcription;
          convDiv.textContent = fullConversation;

          metrics.textContent = `Transcription: ${transcribeTime}ms | Text length: ${transcription.length} chars`;

          // Summarize if we have enough content
          if (fullConversation.length > 50) {
            status.textContent = 'ðŸ”„ Generating summary...';
            const summaryStart = Date.now();
            
            const summaryResult = await summarizer(fullConversation, { 
              max_new_tokens: 50,
              min_length: 10
            });
            
            const summaryTime = Date.now() - summaryStart;
            const totalTime = Date.now() - startTime;
            
            summaryDiv.textContent = summaryResult[0].summary_text;
            metrics.textContent = `Transcription: ${transcribeTime}ms | Summary: ${summaryTime}ms | Total: ${totalTime}ms`;
            status.textContent = 'âœ… Ready for next speech';
          } else {
            status.textContent = 'âœ… Ready for next speech';
          }
        } else {
          status.textContent = 'âš ï¸ No clear speech detected';
        }

      } catch (err) {
        status.textContent = 'âŒ Processing error: ' + err.message;
        console.error(err);
      } finally {
        isProcessing = false;
      }
    }

    function speakFiller(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 1.0;
      speechSynthesis.speak(utterance);
      fillerDiv.textContent = `Filler: "${text}"`;
      setTimeout(() => { fillerDiv.textContent = ''; }, 5000);
    }
  </script>
</body>
</html>
