<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Voice Interview AI Demo - Challenge 1</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 20px; max-width: 900px; line-height: 1.6; background: #f5f5f5; }
    .container { background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
    #status { color: #0066cc; font-weight: bold; margin: 15px 0; min-height: 1.5em; }
    #metrics { color: #555; font-size: 14px; margin: 10px 0; padding: 8px; background: #f0f0f0; border-radius: 4px; }
    button { padding: 10px 20px; margin: 5px 5px 10px 0; font-size: 16px; cursor: pointer; background: #0066cc; color: white; border: none; border-radius: 4px; }
    button:disabled { background: #999; cursor: not-allowed; }
    .filler { color: #e67e22; font-style: italic; font-weight: bold; margin: 10px 0; padding: 8px; background: #fff3e0; border-left: 4px solid #e67e22; border-radius: 4px; }
  </style>

  <script type="importmap">
  {
    "imports": {
      "@xenova/transformers": "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2"
    }
  }
  </script>
</head>
<body>
  <div class="container">
    <h1>Voice Interview AI Demo</h1>
    <p>Goal: &lt;30â€“100 MB model, &lt;50 ms feel, real-time summaries + fillers (offline)</p>

    <button id="loadBtn">Load Models</button>
    <button id="startMicBtn" disabled>Start Mic & VAD</button>
    <button id="stopMicBtn" disabled>Stop Mic</button>

    <div id="status">Status: Waiting...</div>
    <div id="metrics"></div>

    <div id="fillerArea" class="filler"></div>

    <h3>Live Transcription:</h3>
    <div id="conversation">Speak something...</div>

    <h3>Live Summary:</h3>
    <div id="summary">Summary will appear after you speak and pause...</div>
  </div>

  <script type="module">
    import { pipeline, env } from '@xenova/transformers';
    import { loadWhisper, transcribeChunk } from './whisper.js';

    env.logLevel = 'silent';
    env.allowLocalModels = false;

    const status = document.getElementById('status');
    const metrics = document.getElementById('metrics');
    const convDiv = document.getElementById('conversation');
    const summaryDiv = document.getElementById('summary');
    const fillerDiv = document.getElementById('fillerArea');

    const loadBtn = document.getElementById('loadBtn');
    const startMicBtn = document.getElementById('startMicBtn');
    const stopMicBtn = document.getElementById('stopMicBtn');

    let summarizer = null;
    let transcriber = null;
    let audioContext = null;
    let stream = null;
    let processor = null;
    let lastSpeechTime = Date.now();
    let fillerCooldown = false;
    let currentChunk = [];
    let fullConversation = '';

    loadBtn.addEventListener('click', async () => {
      loadBtn.disabled = true;
      status.textContent = 'Loading summarizer + Whisper-tiny...';

      try {
        summarizer = await pipeline('summarization', 'Xenova/t5-small', {
          device: 'webgpu',
          quantized: true
        });

        transcriber = await loadWhisper();

        status.textContent = 'âœ… All models loaded! Click Start Mic.';
        startMicBtn.disabled = false;
      } catch (err) {
        status.textContent = 'âŒ Load failed: ' + err.message;
        console.error(err);
      } finally {
        loadBtn.disabled = false;
      }
    });

    startMicBtn.addEventListener('click', async () => {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        processor = audioContext.createScriptProcessor(512, 1, 1);

        source.connect(processor);
        processor.connect(audioContext.destination);

        processor.onaudioprocess = (e) => {
          const input = e.inputBuffer.getChannelData(0);
          let sum = 0;
          for (let i = 0; i < input.length; i++) sum += input[i] * input[i];
          const energy = sum / input.length;

          if (energy > 0.001) {
            lastSpeechTime = Date.now();
            currentChunk.push(...input);
          } else {
            const silenceMs = Date.now() - lastSpeechTime;
            if (silenceMs > 3000 && !fillerCooldown) {
              speakFiller("Okay... let me process that.");
              fillerCooldown = true;
              setTimeout(() => fillerCooldown = false, 6000);

              if (currentChunk.length > 4000 && transcriber) {
                const buffer = new Float32Array(currentChunk);
                processAudioChunk(buffer);
              }
              currentChunk = [];
            }
          }
        };

        status.textContent = 'ðŸŽ¤ Listening... Speak clearly, then pause 4+ seconds';
        startMicBtn.disabled = true;
        stopMicBtn.disabled = false;
      } catch (err) {
        status.textContent = 'âŒ Mic error: ' + err.message;
        console.error(err);
      }
    });

    stopMicBtn.addEventListener('click', () => {
      if (stream) stream.getTracks().forEach(t => t.stop());
      if (processor) processor.disconnect();
      if (audioContext) audioContext.close();

      status.textContent = 'Mic stopped.';
      startMicBtn.disabled = false;
      stopMicBtn.disabled = true;
    });

    async function processAudioChunk(buffer) {
      try {
        const text = await transcribeChunk(transcriber, buffer);
        if (text && text.length > 5) {
          fullConversation += (fullConversation ? ' ' : '') + text;
          convDiv.textContent = fullConversation;

          const output = await summarizer(text, { max_new_tokens: 40 });
          summaryDiv.textContent = output[0].summary_text || '(summary generated)';
        }
      } catch (err) {
        console.error('Transcription/summary error:', err);
        summaryDiv.textContent = '(error during processing)';
      }
    }

    function speakFiller(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.rate = 1.0;
      speechSynthesis.speak(utterance);
      fillerDiv.textContent = `Filler: "${text}"`;
      setTimeout(() => { fillerDiv.textContent = ''; }, 5000);
    }
  </script>
</body>
</html>